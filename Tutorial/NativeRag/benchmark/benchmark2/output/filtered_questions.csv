"context","question","answer","source_doc","groundedness_score","groundedness_eval","relevance_score","relevance_eval","standalone_score","standalone_eval"
"```bash
python -m python_coreml_stable_diffusion.torch2coreml \\
    --model-version prompthero/openjourney-v4 \\
    --convert-unet \\
    --convert-text-encoder \\
    --convert-vae-decoder \\
    --convert-vae-encoder \\
    --convert-safety-checker \\
    --quantize-nbits 6 \\
    --attention-implementation ORIGINAL \\
    --compute-unit CPU_AND_GPU \\
    --bundle-resources-for-swift-cli \\
    --check-output-correctness \\
    -o models/original/openjourney-6-bit
```

<br>
<div style=""background-color: #f0fcf0; padding: 8px 32px 1px; outline: 1px solid; border-radius: 10px;"">

* Use `--convert-vae-encoder` if you want to use image-to-image tasks.
* Do _not_ use `--chunk-unet` with `--quantized-nbits 6` (or less), as the quantized model is small enough to work fine on both iOS and macOS.
</div>
<br>

3. Repeat the conversion for the `SPLIT_EINSUM_V2` attention implementation:

```bash
python -m python_coreml_stable_diffusion.torch2coreml \\
    --model-version prompthero/openjourney-v4 \\
    --convert-unet \\
    --convert-text-encoder \\
    --convert-vae-decoder \\
    --convert-safety-checker \\
    --quantize-nbits 6 \\
    --attention-implementation SPLIT_EINSUM_V2 \\
    --compute-unit ALL \\
    --bundle-resources-for-swift-cli \\
    --check-output-correctness \\
    -o models/split_einsum_v2/openjourney-6-bit
```

4. Test the converted models on the desired hardware. As a rule of thumb, the `ORIGINAL` version usually works better on macOS, whereas `SPLIT_EINSUM_V2` is usually faster on iOS. For more details and additional data points, see [these tests contributed by the community](https://github.com/huggingface/swift-coreml-diffusers/issues/31) on the previous version of Stable Diffusion for Core ML.","What compute unit is used for the SPLIT_EINSUM_V2 attention implementation conversion?
","ALL","huggingface/blog/blob/main/fast-diffusers-coreml.md","5","The context clearly specifies the compute unit used for the `SPLIT_EINSUM_V2` attention implementation conversion in the command line argument `--compute-unit ALL`. This information is directly provided and unambiguous.
","3","The question is highly specific to a particular implementation detail (SPLIT_EINSUM_V2 attention) within the Hugging Face ecosystem, which is relevant to developers optimizing or customizing transformer models. However, it is niche and likely only useful to a small subset of advanced users working on low-level optimizations or hardware-specific implementations.  
","5","The question contains technical terms like ""SPLIT_EINSUM_V2"" and ""attention implementation conversion,"" which are specific and may require domain knowledge or documentation to understand fully. However, the question is self-contained and does not rely on an implicit context or external reference. It is clear what is being asked, even if the terms are specialized.

"

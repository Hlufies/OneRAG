{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/deepseek/bin/python\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"TRANSFORMERS_CACHE\"] = \"/newdata/HJQ/Cultural_Tourism_Program/RAGDemo/models\"\n",
    "os.environ[\"HF_ENDPOINT\"] = \"https://hf-mirror.com\"\n",
    "os.environ[\"OLLAMA_MODELS\"] = \"/newdata/HTY/BIO_LLM/DS_70b/models\"\n",
    "os.environ[\"OLLAF_ENDPOINT\"] = \"https://hf-mirror.com\"\n",
    "os.environ[\"HF_ENDPOINT\"] = \"https://hf-mirror.com\"\n",
    "os.environ[\"HF_HOME\"] = \"/newdata/HJQ/Cultural_Tourism_Program/RAGDemo/models\"\n",
    "os.environ[\"TRANSFORMERS_OFFLINE\"] = \"1\"\n",
    "\n",
    "import sys\n",
    "print(sys.executable)  # 应显示虚拟环境路径而非系统路径"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/deepseek/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/root/anaconda3/envs/deepseek/lib/python3.10/site-packages/transformers/utils/hub.py:105: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n",
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.40it/s]\n",
      "Some weights of Blip2ForImageTextRetrieval were not initialized from the model checkpoint at Salesforce/blip2-opt-2.7b and are newly initialized: ['embeddings.position_embeddings.weight', 'embeddings.word_embeddings.weight', 'itm_head.bias', 'itm_head.weight', 'text_projection.bias', 'text_projection.weight', 'vision_projection.bias', 'vision_projection.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"TRANSFORMERS_CACHE\"] = \"/newdata/HJQ/Cultural_Tourism_Program/RAGDemo/models\"\n",
    "os.environ[\"HF_ENDPOINT\"] = \"https://hf-mirror.com\"\n",
    "os.environ[\"OLLAMA_MODELS\"] = \"/newdata/HTY/BIO_LLM/DS_70b/models\"\n",
    "os.environ[\"OLLAF_ENDPOINT\"] = \"https://hf-mirror.com\"\n",
    "os.environ[\"HF_ENDPOINT\"] = \"https://hf-mirror.com\"\n",
    "os.environ[\"HF_HOME\"] = \"/newdata/HJQ/Cultural_Tourism_Program/RAGDemo/models\"\n",
    "os.environ[\"TRANSFORMERS_OFFLINE\"] = \"1\"\n",
    "\n",
    "from transformers import AutoProcessor, Blip2ForConditionalGeneration,  Blip2ForImageTextRetrieval\n",
    "import torch\n",
    "from PIL import Image\n",
    "\n",
    "# 加载模型和处理器\n",
    "device = \"cuda:0\"\n",
    "processor = AutoProcessor.from_pretrained(\"Salesforce/blip2-opt-2.7b\", cache_dir=\"/newdata/HJQ/Cultural_Tourism_Program/RAGDemo/models\")\n",
    "model = Blip2ForImageTextRetrieval.from_pretrained(\n",
    "    \"Salesforce/blip2-opt-2.7b\", \n",
    "    torch_dtype=torch.float16,  # 半精度节省显存\n",
    "    device_map=device,          # 自动分配设备（CPU/GPU）\n",
    "    cache_dir=\"/newdata/HJQ/Cultural_Tourism_Program/RAGDemo/models\"  # 指定缓存目录\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = Image.open(\"/newdata/HJQ/Cultural_Tourism_Program/RAGDemo/OneRAG/Tutorial/NativeRAG/dataset/imgs/R0000019.png\")\n",
    "question = \"Question: What is the cat doing? Answer:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 257, 1408])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = processor(image, return_tensors=\"pt\").to(device, torch.float16)\n",
    "# inputs = processor(image, return_tensors=\"pt\").to(device)\n",
    "img = model.vision_model(**inputs)\n",
    "img.last_hidden_state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.1410, -0.0515, -0.3416,  ..., -0.3865,  0.0906,  0.4053],\n",
      "         [ 0.4800,  0.2905, -0.0299,  ..., -0.2404,  0.8228,  0.4644],\n",
      "         [ 1.0273, -0.6450, -0.4998,  ...,  0.4060, -0.1692,  0.3262],\n",
      "         ...,\n",
      "         [ 0.5498,  0.3965,  0.4319,  ...,  0.1715, -0.0894,  0.2898],\n",
      "         [-1.1221,  1.1895, -0.2715,  ...,  0.6948, -0.8276,  0.8906],\n",
      "         [ 0.3005,  0.0367, -0.1541,  ..., -0.3083,  0.8696,  0.5493]]],\n",
      "       device='cuda:0', dtype=torch.float16, grad_fn=<NativeLayerNormBackward0>)\n",
      "torch.Size([1, 257, 1408])\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')\n",
      "torch.Size([1, 257])\n"
     ]
    }
   ],
   "source": [
    "image_embeds = img[0]\n",
    "image_attention_mask = torch.ones(image_embeds.size()[:-1], dtype=torch.long, device=image_embeds.device)\n",
    "\n",
    "print(image_embeds)\n",
    "print(image_embeds.shape)\n",
    "\n",
    "print(image_attention_mask)\n",
    "print(image_attention_mask.shape)\n",
    "query_tokens = model.query_tokens.expand(image_embeds.shape[0], -1, -1)\n",
    "query_outputs = model.qformer(\n",
    "    query_embeds=query_tokens,\n",
    "    encoder_hidden_states=image_embeds,\n",
    "    encoder_attention_mask=image_attention_mask,\n",
    "    return_dict=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[-0.0061,  0.2229,  0.1544,  ..., -0.4864, -0.1867, -0.4073],\n",
      "         [ 1.6960,  0.0082, -0.6006,  ..., -0.7425,  1.0083, -0.6608],\n",
      "         [-0.9796,  0.1995, -0.1847,  ..., -1.3880, -0.4337,  1.0066],\n",
      "         ...,\n",
      "         [-0.8283, -0.4150, -0.0110,  ..., -0.3855,  0.2681,  0.7748],\n",
      "         [-0.7986,  0.2922,  0.5896,  ..., -0.6854, -1.5820, -0.1256],\n",
      "         [ 0.0854,  0.4348, -0.9331,  ..., -1.7016,  0.2088, -0.5576]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>), pooler_output=tensor([[-6.1461e-03,  2.2294e-01,  1.5441e-01, -3.2191e-01, -2.3167e-01,\n",
      "          6.3223e-01, -2.2285e-01, -6.6286e-01, -2.5129e-02,  3.7720e-01,\n",
      "         -2.2187e-01, -1.3422e-01, -6.2565e-01,  2.9700e-01, -3.6429e-01,\n",
      "          4.2409e-01,  5.7565e-01, -1.4399e-01,  7.5582e-01,  4.2788e-02,\n",
      "          1.4999e-01, -4.3969e-01, -1.3227e-01,  6.3375e-02,  1.3900e+00,\n",
      "          5.9420e-01,  9.2109e-01, -1.2041e-02, -2.5651e-01,  9.5452e-02,\n",
      "         -2.2242e-01, -6.2620e-02, -3.4282e-01, -3.1470e-01, -4.2190e-01,\n",
      "         -4.8074e-01, -5.0090e-01, -5.0947e-01,  2.8906e-01,  1.0344e-01,\n",
      "          3.5870e-01,  1.1511e+00,  3.9565e-01,  3.5454e-01,  1.2723e-01,\n",
      "          2.9953e-01, -5.2740e-01, -7.1274e-03,  3.5007e-03, -7.3130e-01,\n",
      "         -1.5981e-01, -6.6977e-01,  7.9893e-01, -7.6891e-02,  9.0855e-02,\n",
      "         -3.4363e-01, -1.2640e+00, -2.6316e-01,  5.7009e-01, -7.2140e-01,\n",
      "         -1.9574e-01,  1.9834e-01,  1.0320e+00,  2.0221e-01, -6.5007e-01,\n",
      "          3.0660e-01, -2.4148e-01,  6.3576e-02,  2.6631e-02, -1.2322e-02,\n",
      "         -1.0471e+00,  7.4829e-01,  8.9334e-01, -2.7192e-01,  4.0921e-01,\n",
      "         -5.6712e-02, -3.4118e-01, -5.0929e-02, -5.7718e-01,  6.6542e-01,\n",
      "         -5.7663e-01, -8.3488e-01,  3.2762e-01, -4.0622e-01, -3.3526e-01,\n",
      "          1.8738e-01, -1.6038e+00, -5.0350e-02,  2.5383e-01, -1.3984e-01,\n",
      "         -3.9112e-01, -7.6463e-01, -2.9534e-01,  1.4047e-01, -6.4868e-01,\n",
      "         -2.6958e-01,  1.3413e-01,  1.5798e-01,  3.5434e-01, -7.9417e-01,\n",
      "          3.1024e-01, -8.2470e-03, -1.6947e-01, -1.5367e-01,  1.9183e-01,\n",
      "          4.8787e-02, -2.1736e-01, -2.9147e-01, -5.2432e-01,  4.6498e-01,\n",
      "          8.4483e-01,  5.1292e-01, -6.3008e-01, -2.3970e-01,  8.9259e-02,\n",
      "          1.1473e+00, -1.4081e-01, -7.1300e-01,  6.5152e-01, -4.9520e-01,\n",
      "         -8.3569e-01,  2.2716e-01, -5.9404e-01,  9.5452e-02,  5.6994e-01,\n",
      "          2.1489e-01,  4.4111e-01, -3.0855e-01,  1.9088e-01,  4.0944e-01,\n",
      "         -3.6882e-01, -3.0098e-01,  7.5122e-01,  3.4255e-01,  4.1452e-02,\n",
      "         -4.9544e-01,  7.9724e-01, -7.1096e-01, -1.7568e-01, -1.1542e-01,\n",
      "          6.7787e-01, -1.5889e-01, -1.9597e-02,  1.5442e-01,  2.0632e-01,\n",
      "          7.4452e-02, -2.8967e-01,  5.5841e-02, -7.5929e-02, -3.1602e-01,\n",
      "          1.3088e-01, -5.0433e-01, -7.6698e-01, -1.5006e-01, -5.9717e-01,\n",
      "         -3.8002e-01,  4.9316e-01,  4.3286e-01, -4.9740e-01, -1.2317e-01,\n",
      "          5.8239e-01, -7.1672e-02, -4.3734e-01, -4.0225e-01, -3.6339e-01,\n",
      "          8.7438e-02,  2.7074e-01, -4.6351e-01, -1.3712e+00,  3.3037e-01,\n",
      "         -2.7671e-01,  6.0093e-01, -3.0052e-01, -4.3140e-01,  5.9063e-01,\n",
      "          5.1920e-01,  2.7075e-01,  8.0162e-02,  2.9954e-01, -4.6041e-01,\n",
      "          6.4809e-01,  5.8570e-01, -3.3318e-01,  5.0345e-02, -3.7556e-02,\n",
      "          3.1966e-01,  4.4820e-01,  3.9471e-02,  2.6468e-01, -3.8761e-02,\n",
      "         -1.1694e-02, -9.3606e-01, -8.5397e-01, -4.9539e-01,  9.7532e-01,\n",
      "         -4.7705e-01,  2.8860e-02,  1.6326e-01,  1.9347e-01, -8.5656e-01,\n",
      "         -5.8223e-01, -2.3092e-01, -6.5287e-01, -1.2501e-01,  3.6463e-01,\n",
      "          6.3602e-01, -4.8221e-01,  4.0602e-01,  8.0405e-01,  5.0956e-01,\n",
      "         -5.7833e-01,  1.7361e-01, -1.0446e+00,  2.8280e-01, -6.2280e-01,\n",
      "          3.1263e-01,  5.4243e-01, -1.2107e-01,  4.8778e-01,  7.8690e-01,\n",
      "         -3.6547e-01,  4.5497e-01,  4.0960e-01, -3.7627e-02,  9.5756e-01,\n",
      "          2.3243e-01,  7.7840e-01, -1.7492e-01,  6.6121e-01, -7.0443e-01,\n",
      "         -4.2800e-01, -3.8312e-01, -1.4100e-02, -1.5749e-01,  3.4652e-02,\n",
      "         -8.1137e-02,  1.8458e-01, -4.7312e-01,  1.0623e-01, -4.5065e-01,\n",
      "         -7.7432e-01, -2.4975e-01, -2.0213e-01,  7.5479e-01, -4.8646e-01,\n",
      "         -1.1116e-03,  7.4639e-01, -5.1311e-01, -6.4042e-01,  1.9690e-02,\n",
      "          2.3304e-01, -1.3759e-01,  7.7717e-01, -4.2970e-01, -7.9679e-02,\n",
      "          8.8207e-01,  1.9612e-01,  1.1277e-03, -5.2155e-01,  5.0739e-01,\n",
      "          6.7570e-02, -5.3608e-02, -7.5632e-01, -4.4559e-01, -3.8850e-01,\n",
      "         -8.2788e-01,  1.0157e+00, -2.5214e-01,  2.4084e-01,  1.5237e-01,\n",
      "          3.9420e-01, -3.2465e-02,  2.6737e-01,  6.0614e-01, -2.6953e-01,\n",
      "          2.0160e-01,  5.6766e-01,  4.9988e-01, -2.7869e-01,  8.7131e-01,\n",
      "         -3.3513e-01, -4.8953e-01, -6.2866e-01, -5.4695e-01,  1.5996e-01,\n",
      "          1.6239e-01,  1.4088e+00,  3.1918e-01,  7.8045e-01, -3.4359e-01,\n",
      "         -4.2316e-01, -3.1693e-01, -5.7053e-01,  2.4043e-01, -9.0576e-02,\n",
      "         -1.6011e-01,  8.4989e-02,  4.8884e-02, -1.1629e-01,  8.2455e-01,\n",
      "         -2.8627e-01,  4.3979e-03,  3.2952e-01,  1.5984e-01,  2.4124e-01,\n",
      "         -8.2452e-01,  9.5285e-01,  6.6209e-02, -3.5414e-02, -1.3030e-01,\n",
      "         -3.9200e-01, -1.1424e+00,  2.7927e-01,  3.7978e-01, -1.7592e-02,\n",
      "         -6.5732e-03,  8.9446e-01, -9.9907e-01, -1.1365e+00,  5.4227e-01,\n",
      "          5.7592e-03,  1.6575e-01, -3.9021e-01, -6.3245e-01,  7.0404e-01,\n",
      "         -3.0780e-01, -6.1345e-02, -3.1043e-01, -4.8986e-02,  4.8384e-02,\n",
      "          2.1130e-01,  1.2373e+00, -5.5839e-02,  1.4229e+00,  7.2900e-01,\n",
      "         -6.5697e-01, -4.2513e-01,  9.9292e-02, -6.5567e-01, -4.7067e-01,\n",
      "         -5.1388e-01, -1.9031e-02,  2.4423e-01,  4.4438e-01, -5.1641e-01,\n",
      "          5.9305e-01,  1.6260e-01, -4.6376e-01,  2.1997e-01, -2.8560e-01,\n",
      "          6.6526e-01,  1.8381e-01, -3.0640e-01,  1.2966e+00, -6.2292e-01,\n",
      "         -9.5791e-01, -1.2019e-01,  8.7291e-02,  1.5381e+00,  2.4690e-02,\n",
      "         -7.2563e-01,  2.9635e-01, -6.9252e-01, -3.7267e-02, -7.3623e-01,\n",
      "         -6.0962e-01, -7.1759e-02,  7.3962e-01, -4.6737e-01, -4.4835e-01,\n",
      "         -3.7268e-01, -1.2733e-02,  1.5048e-01,  4.0275e-01,  2.1708e-01,\n",
      "         -2.2539e-01,  3.5074e-01,  1.6969e-01, -4.1296e-01,  1.5529e+00,\n",
      "         -9.8479e-01,  1.3268e-01, -7.3120e-02, -1.0095e-02,  1.9836e-01,\n",
      "          5.5710e-01, -2.4380e-01,  4.5158e-02, -9.9343e-01, -1.8604e-01,\n",
      "         -3.5796e-01,  4.2280e-02,  9.5148e-01,  4.6719e-01, -4.8715e-01,\n",
      "          6.7058e-02,  1.0656e-01,  7.9340e-02, -6.5737e-01,  3.8391e-01,\n",
      "          3.0651e-01, -5.3997e-01, -7.0765e-04, -1.0626e-01, -7.5374e-01,\n",
      "          1.7253e-01, -5.9678e-02,  7.8624e-02,  7.6079e-01, -2.7018e-01,\n",
      "         -3.2341e-02,  2.8571e-01, -1.0494e+00, -1.4050e-01,  2.8327e-01,\n",
      "          2.3017e-01,  4.8716e-01,  1.2819e-01,  6.2664e-01, -2.3750e-01,\n",
      "         -3.3134e-01, -7.3364e-01, -2.2342e-01, -4.6310e-01,  3.6997e-01,\n",
      "          3.2400e-01,  4.1174e-01, -7.9392e-01,  1.5361e-01, -1.6054e-01,\n",
      "          3.8478e-01, -1.3570e-01, -6.5745e-01,  4.3153e-01,  1.1015e+00,\n",
      "         -3.4378e-01,  4.9207e-01,  5.1729e-01,  3.7785e-01,  9.5763e-01,\n",
      "         -6.6004e-01,  1.3250e-01, -5.0421e-01, -2.6507e-02,  1.2194e+00,\n",
      "         -1.1775e+00,  3.4709e-01, -9.6111e-01, -4.3614e-01, -7.4893e-01,\n",
      "          6.5852e-01,  4.8856e-01, -1.8093e-01, -3.9811e-01,  4.7972e-01,\n",
      "          7.4835e-01,  1.4731e-01,  1.9979e-01,  1.7540e-01,  2.8727e-01,\n",
      "          9.5911e-01,  2.6589e-01, -7.8381e-01, -9.9987e-02,  3.3790e-01,\n",
      "          1.8253e-01, -5.0295e-01,  2.3370e-01, -2.2218e-01, -6.4831e-01,\n",
      "          5.3964e-01, -4.8515e-01,  9.9039e-01, -4.2171e-01,  5.2835e-01,\n",
      "          3.4860e-02,  6.7235e-02,  1.0817e+00, -9.6241e-01,  9.1495e-03,\n",
      "          2.4478e-01, -2.0869e-01, -4.2250e-01, -2.6608e-01, -4.8744e-01,\n",
      "         -1.0309e+00,  3.4598e-01,  8.4911e-01,  1.8716e+00, -1.8609e-01,\n",
      "         -1.1634e-01, -5.4807e-01,  6.8201e-01,  7.7799e-01,  1.0875e-02,\n",
      "         -3.4175e-02, -4.1225e-01,  2.5987e-02, -1.2296e-01, -3.5454e-01,\n",
      "          3.8156e-01,  4.9662e-01,  3.2892e-02,  1.0945e-01, -3.7892e-01,\n",
      "          5.6839e-01, -4.4614e-01,  1.2762e-01, -4.2073e-01,  9.3719e-01,\n",
      "          8.8120e-02, -3.4475e-01, -2.3223e-01, -6.4294e-01, -7.9698e-01,\n",
      "          5.3604e-01,  4.4377e-01, -4.4603e-02,  3.2795e-01, -1.4654e+00,\n",
      "          8.1758e-01,  5.8566e-01,  3.9249e-02,  5.1247e-01,  3.0127e-01,\n",
      "         -7.5536e-01, -1.3832e-01, -2.8417e-01, -7.2090e-01,  1.8522e-01,\n",
      "         -3.6644e-02, -4.2777e-02, -3.5754e-01, -3.8862e-01,  3.6639e-01,\n",
      "          2.1270e-01, -1.0800e-01,  3.1081e-01,  5.3803e-01,  9.0137e-01,\n",
      "         -6.4548e-01, -6.5620e-01,  3.1332e-01, -7.6984e-01,  4.2002e-01,\n",
      "         -3.6644e-01, -6.4758e-01,  7.0601e-02,  2.4245e-01,  1.9827e-01,\n",
      "          1.3468e+00,  4.9199e-02,  2.9626e-01,  6.3525e-01, -2.7829e-02,\n",
      "          6.0290e-02,  2.4865e-01,  1.0137e-01, -5.4595e-01,  8.6784e-02,\n",
      "         -2.5964e-01, -5.7130e-01,  1.9178e-02, -1.7917e-01,  2.3377e-01,\n",
      "         -2.2024e-01,  1.6436e-02, -1.6948e-01, -9.8867e-01,  6.3361e-02,\n",
      "          1.1505e+00, -8.0908e-01,  6.7614e-01,  2.7556e-01, -5.6021e-02,\n",
      "          3.3280e-01,  2.4492e-01, -6.9183e-01,  5.7768e-02, -4.9986e-01,\n",
      "         -3.3863e-01,  7.0703e-01,  6.1378e-02, -9.0160e-01, -2.9071e-01,\n",
      "         -6.2172e-01, -9.4613e-02, -7.2551e-01,  7.3281e-01,  5.6939e-01,\n",
      "          4.2349e-01,  1.8178e-01,  2.2386e-01, -3.0798e-03,  1.0777e+00,\n",
      "         -1.3575e-01,  1.8891e-01,  5.1507e-02, -2.8314e-01, -4.6393e-01,\n",
      "         -3.2326e-01, -2.3537e-01, -1.7680e-01,  1.7291e-01,  2.2109e-01,\n",
      "          1.1574e+00,  1.8319e+00,  3.9712e-01, -1.0680e+00,  2.0466e-01,\n",
      "         -6.2384e-02,  3.5388e-01,  2.9278e-01, -8.2322e-01,  3.9821e-01,\n",
      "          2.5415e-01, -6.5049e-01, -2.7292e-01, -4.2715e-01, -8.9914e-01,\n",
      "          1.1587e-01, -5.6376e-01, -6.4101e-02,  2.2026e-01,  5.2940e-01,\n",
      "         -2.6317e-01,  1.3660e+00, -8.2312e-01,  9.6951e-01,  6.1758e-01,\n",
      "          1.0519e+00, -4.1581e-01, -1.5784e-01, -2.8154e-01, -3.8396e-01,\n",
      "         -9.8733e-02,  5.5600e-01, -1.0415e-01, -5.1767e-01, -2.0702e-01,\n",
      "         -1.1711e-02,  7.3241e-01, -6.2301e-01, -5.3660e-01,  2.6526e-01,\n",
      "         -8.2811e-01,  2.0212e-01,  2.8753e-01,  1.4064e-01,  1.8710e-01,\n",
      "         -1.2168e-01, -4.9140e-01,  6.3091e-01,  3.4180e-01, -3.8302e-01,\n",
      "         -7.9796e-01, -8.0061e-01,  1.1907e-01,  8.8151e-01, -7.4591e-01,\n",
      "         -1.2483e-01, -3.2446e-02, -3.2356e-01, -4.9019e-02,  3.9862e-02,\n",
      "         -2.5584e-01, -7.3364e-01,  4.4512e-01,  3.3374e-01,  8.9347e-01,\n",
      "          3.2035e-01, -9.0021e-01,  3.7873e-01,  2.7007e-01,  1.3574e+00,\n",
      "          2.5026e-01, -1.7901e-01,  2.1968e-01,  3.5487e-01,  1.6058e-01,\n",
      "         -4.8445e-02,  5.4253e-01,  7.0046e-01, -2.2605e-01, -6.9372e-02,\n",
      "          1.9657e-01,  4.6017e-01, -4.9472e-01, -1.7530e-01,  2.1366e-01,\n",
      "          6.9824e-02, -4.2216e-01, -7.3092e-03,  1.1460e-01, -2.3373e-02,\n",
      "          8.3311e-01, -3.2832e-01, -4.9469e-01, -1.0519e-01, -6.6671e-01,\n",
      "          1.1174e+00,  6.0113e-01,  1.6310e-01,  2.9281e-01,  4.2059e-01,\n",
      "         -5.3336e-01,  1.1585e+00,  4.9129e-02,  4.2851e-01,  3.2031e-01,\n",
      "         -4.0354e-01, -2.3394e-01,  7.0795e-01,  3.3056e-01, -3.4750e-01,\n",
      "          6.9202e-01,  1.0782e-01,  2.9039e-01,  1.1289e-01, -1.0330e+00,\n",
      "          6.5167e-02, -5.7960e-02,  6.5055e-02, -8.9661e-01,  3.8277e-01,\n",
      "          1.1806e-01,  6.6662e-01,  3.2676e-01,  1.3927e-01, -1.1084e-01,\n",
      "         -2.2112e-01, -3.7450e-01, -8.8768e-03,  1.6041e+00, -3.7470e-02,\n",
      "         -9.5417e-02, -1.5440e-01, -3.3147e-01, -5.0555e-03,  1.7922e-02,\n",
      "         -3.4355e-01, -7.8425e-01,  1.1038e-01, -1.3914e-01, -1.3892e-01,\n",
      "         -6.3990e-01,  1.4934e-01, -5.3183e-01,  2.6612e-02,  9.4405e-02,\n",
      "         -5.5711e-01, -4.0517e-01,  8.7118e-01,  2.0385e-01, -1.9287e-01,\n",
      "          1.3433e-01,  2.4328e-02,  4.2470e-01,  3.4879e-01,  2.2616e-01,\n",
      "          2.0246e-01, -7.4054e-01, -1.6543e-01, -1.1955e+00, -3.0859e-01,\n",
      "         -4.8637e-01, -1.8672e-01, -4.0733e-01]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)\n",
      "torch.Size([1, 32, 768])\n",
      "torch.Size([1, 32, 768])\n"
     ]
    }
   ],
   "source": [
    "print(query_outputs)\n",
    "image_embeds = query_outputs[0]\n",
    "print(image_embeds.shape)\n",
    "print(query_outputs.last_hidden_state.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = processor.tokenizer\n",
    "inputs = tokenizer(\n",
    "    question,\n",
    "    return_tensors=\"pt\",\n",
    "    truncation=True,\n",
    "    max_length=32\n",
    ")\n",
    "\n",
    "input_ids, attention_mask = inputs.input_ids.to(device), inputs.attention_mask.to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    2, 45641,    35,   653,    16,     5,  4758,   608,   116, 31652,\n",
      "            35]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(input_ids)\n",
    "# query_embeds = model.embeddings(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = model.config.text_config.vocab_size  # BLIP-2 的文本编码器词汇量\n",
    "assert input_ids.max() < vocab_size, \"输入ID值超出词汇表范围\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m----> 2\u001b[0m     query_embeds \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(query_embeds\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# query_embeds = model.embeddings(input_ids)\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# print(query_embeds.shape)\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/deepseek/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/deepseek/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/anaconda3/envs/deepseek/lib/python3.10/site-packages/transformers/models/blip_2/modeling_blip_2.py:1235\u001b[0m, in \u001b[0;36mBlip2TextEmbeddings.forward\u001b[0;34m(self, input_ids, position_ids, query_embeds)\u001b[0m\n\u001b[1;32m   1233\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m input_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1234\u001b[0m     input_ids \u001b[38;5;241m=\u001b[39m input_ids\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mword_embeddings\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m-> 1235\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mword_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1236\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mposition_embedding_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mabsolute\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1237\u001b[0m         position_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mposition_embeddings(position_ids)\n",
      "File \u001b[0;32m~/anaconda3/envs/deepseek/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/deepseek/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/anaconda3/envs/deepseek/lib/python3.10/site-packages/torch/nn/modules/sparse.py:190\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    192\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    193\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    194\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    195\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    196\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/deepseek/lib/python3.10/site-packages/torch/nn/functional.py:2551\u001b[0m, in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2545\u001b[0m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[1;32m   2546\u001b[0m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[1;32m   2547\u001b[0m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[1;32m   2548\u001b[0m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[1;32m   2549\u001b[0m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[1;32m   2550\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[0;32m-> 2551\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    query_embeds = model.embeddings(input_ids)\n",
    "    print(query_embeds.shape)\n",
    "# query_embeds = model.embeddings(input_ids)\n",
    "# print(query_embeds.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m text_query_tokens \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mquery_tokens\u001b[38;5;241m.\u001b[39mexpand(input_ids\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# 扩展标准查询向量\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m text_attention_mask \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mones\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext_query_tokens\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# 创建文本查询掩码\u001b[39;00m\n\u001b[1;32m      3\u001b[0m combined_attention_mask \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([text_attention_mask, attention_mask], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# 需拼接掩码\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "text_query_tokens = model.query_tokens.expand(input_ids.shape[0], -1, -1)  # 扩展标准查询向量\n",
    "text_attention_mask = torch.ones(text_query_tokens.size()[:-1],)  # 创建文本查询掩码\n",
    "combined_attention_mask = torch.cat([text_attention_mask, attention_mask], dim=1)  # 需拼接掩码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'self' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m text_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241m.\u001b[39mqformer(\n\u001b[1;32m      2\u001b[0m     query_embeds\u001b[38;5;241m=\u001b[39mtext_query_tokens,\n\u001b[1;32m      3\u001b[0m     query_length\u001b[38;5;241m=\u001b[39mtext_query_tokens\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m],  \u001b[38;5;66;03m# 使用实际查询长度\u001b[39;00m\n\u001b[1;32m      4\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mcombined_attention_mask,\n\u001b[1;32m      5\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[1;32m      6\u001b[0m )\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# query_tokens = model.query_tokens.expand(image_embeds.shape[0], -1, -1)\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# query_attention_mask = torch.ones(query_tokens.size()[:-1], dtype=torch.long, device=query_tokens.device)\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# attention_mask = torch.cat([query_attention_mask, attention_mask], dim=1)\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# query_embeds = model.embeddings(\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m#             input_ids=input_ids,\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m#             query_embeds=query_tokens)\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'self' is not defined"
     ]
    }
   ],
   "source": [
    "text_outputs = self.qformer(\n",
    "    query_embeds=text_query_tokens,\n",
    "    query_length=text_query_tokens.shape[1],  # 使用实际查询长度\n",
    "    attention_mask=combined_attention_mask,\n",
    "    return_dict=return_dict,\n",
    ")\n",
    "\n",
    "\n",
    "# query_tokens = model.query_tokens.expand(image_embeds.shape[0], -1, -1)\n",
    "# query_attention_mask = torch.ones(query_tokens.size()[:-1], dtype=torch.long, device=query_tokens.device)\n",
    "# attention_mask = torch.cat([query_attention_mask, attention_mask], dim=1)\n",
    "# query_embeds = model.embeddings(\n",
    "#             input_ids=input_ids,\n",
    "#             query_embeds=query_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepseek",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
